{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcbcb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import featuretools as ft\n",
    "from featuretools.primitives import RollingMean, NumericLag, RollingMin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8104f18",
   "metadata": {},
   "source": [
    "# Feature Engineering for Time Series Problems"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9cd9cb82",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. note::\n",
    "        This guide focuses on feature engineering for single-table time series problems; it does not cover how to handle temporal multi-table data for other machine learning problem types. A more general guide on handling time in Featuretools can be found [here]()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cb8ff8",
   "metadata": {},
   "source": [
    "- Explain time series problem and how it presents a different feature engineering than other problem types.\n",
    "Time series forecasting is different from other machine learning problems in that there is an inherent temporal ordering to the data. The ordering comes from a time index column, so at a specific point in time, we may have knowlege about earlier observations but not later ones. If the data is unordered, itâ€™d be hard to see any overall trend or seasonality, but when sorted by date, any relationships that exist in the data can be seen and used when making predictions (winter is cold; summer is hot!). Notice how this is different from non-time series data, which can be presented in any order without having an impact on the resulting predictions. Therefore, in time series problems, we need to set a time_index column that is just as important for defining our problem as the target_column is.\n",
    "\n",
    "    In that demo, the primary impact the data's temporal ordering has on modeling occurs in splitting the data into training and test data where we use a cutoff_time to determine when that split occurs and make sure no future values are exposed in the feature engineering process. We'll have to account for the same things in this demo since we have to be very careful when splitting our data to not expose future observations in the feature engineering stage.\n",
    "\n",
    "But once the data is split, the predict-remaining-useful-life demo calculates aggregations over the entire life of the engine up until the last available time. In that case, that makes sense! The goal is to predict one future value (the remaining useful life) per engine, so we should look at its entire available history when making that prediction. There is an entire dataframe of engines for which we're predicting this value, but the individual engines are not dependent on one another.\n",
    "\n",
    "However, in this demo and in many time series problems, we're trying to predict a sequential series of values that are highly dependent on one another. In these cases, we can exploit the fact that more recent observations are more predictive than more distant ones--when trying to determine tomorrow's temperature, knowing today's temperature may be the most predictive piece of information we can get. We also only have one table in our dataset, so any aggregations have to be calculated over earlier data from the same column.\n",
    "\n",
    "- Introduce dataset - weather \n",
    "We'll be working with a temperature dataframe of minimum daily temperatures that includes two columns: `Temp` and `Date`. `Date` is our time index, and `Temp` is our target column, which means that the end-goal for our feature engineering is to help us predict future temperatures. \n",
    "\n",
    "    - This is single table, so the concepts introduced about cutoff times and last time indices do not exist in the same way. A cutoff time assumes that each row exists only after its time index value, not that the feature engineering window could actually be entirely before the instance itself. \n",
    "\n",
    "\n",
    "- Introduce problem \n",
    "**Assumes that the data has evenly spaced intervals - support for unevenly spaced intervals is ongoing**\n",
    "\n",
    "The fact that we can build features from our target column comes from its temporal nature. If we are at a point in time `t`, we have access to information from times less than `t`, and we do not have information from times greater than `t`. Our limitations in feature engineering, then, will come from when exactly before `t` we have access to the data. Consider an example where we're recording data that takes a week to ingest; the earliest data we have access to is from seven days ago, or `t - 7`. We'll call this our `gap`. We also need to determine how far back in time before `t - 7` we can go. Too far back, and we may lose the potency of our recent observations, but too recent, and we may not capture the full spectrum of behaviors displayed by the data. In this example, let's say that we only want to look at 5 days worth of data at a time. We'll call this our `window_length`. \n",
    "\n",
    "With these two parameters (`gap` and `window_length`) set, we define our feature engineering window. We can aggregate features over this window as if it were a child DataFrame. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0215152c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gap = 7\n",
    "window_length = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a324a21",
   "metadata": {},
   "source": [
    "- Introduce features - use pictures with windows? \n",
    "\n",
    "There are three types of primitives we'll focus on for time series problems. One of them will extract features from the time index, and the other two types will extract features from our target column. \n",
    "\n",
    "### Datetime Transform Primitives\n",
    "\n",
    "We need a way of implicating time in our time series features. Yes, using recent temperatures is incredibly predictive in determining future temperatures, but there is also a whole host of historical data suggesting that the month of the year is a pretty good indicator for the temperature outside. However, if we look at the data, we'll see that, though the day changes, the observations are always taken at the same hour, so the `Hour` primitive will not likely be useful. Of course, in a dataset that is measured at an hourly frequency or one more granular, `Hour` may be incrediby predictive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ea6ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_primitives = ['Day', \"Year\", \"Weekday\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb729fd3",
   "metadata": {},
   "source": [
    "### Delaying Primitives\n",
    "\n",
    "The simplest thing we can do with our target column is to build features that are delayed (or lagging) versions of the target column. We'll make one feature per observation in our feature engineering windows, so we'll range over time from `t - gap - window_length` to `t - gap`. \n",
    "\n",
    "For this purpose, we can use our `NumericLag` primitive and create one primitive for each instance in our window. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d09a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "delaying_primitives = [NumericLag(periods=i + gap) for i in range(window_length)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7b3a0c",
   "metadata": {},
   "source": [
    "### Rolling Transform Primitives\n",
    "\n",
    "Since we have access to the entire feature engineering window, we can aggregate over that window. Featuretools has several rolling primitives with which we can achieve this. Here, we'll use the `RollingMean` primitives `RollingMin`, setting the `gap` and `window_length` accordingly. Here, the gap is incredibly important, because when the gap is zero, it means the current observation's taret value is present in the window, which exposes our target.\n",
    "\n",
    "This concern also exists for other primitives that reference earlier values in the dataframe. Because of this, when using primitives for time series feature engineering, one must be incredibly careful to not use primitives on the target column that incorporate the current observation when calculating a feature value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8945b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_mean_primitive = RollingMean(window_length=window_length, \n",
    "                                     gap=gap,\n",
    "                                     min_periods=window_length)\n",
    "\n",
    "rolling_min_primitive = RollingMin(window_length=window_length, \n",
    "                                     gap=gap,\n",
    "                                     min_periods=window_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8df0ab",
   "metadata": {},
   "source": [
    "## Run DFS\n",
    "\n",
    "Now that we've definied our time series primitives, we can pass them into DFS and get our feature matrix! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688b5afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from featuretools.demo.weather import load_weather\n",
    "es = load_weather()\n",
    "\n",
    "es['temperatures'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25179346",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "fm, f = ft.dfs(entityset=es,\n",
    "               target_dataframe_name='temperatures',\n",
    "              trans_primitives = (datetime_primitives + \n",
    "                                  delaying_primitives + \n",
    "                                  [rolling_mean_primitive, rolling_min_primitive])\n",
    "              )\n",
    "\n",
    "fm"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
